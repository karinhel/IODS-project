
# Chapter 4 - Clustering and classification


In this chapter we are going to explore one of R's own data "Boston" of the package MASS. The dataset covers many housing-related topics such as crime rate, taxes, race and pupil-teacher ratio. The primary units of the data are the suburbs of Boston.

First, let's discover the structure of the data. 

```{r, include=FALSE}
library(tidyr)
library(dplyr)
library(ggplot2)
library(MASS)
library(GGally)

```


```{r}

data("Boston")

dim(Boston)
str(Boston)

pairs(Boston, col="darkorchid1")

```

We see that there are 506 observations of 14 variables that are mostly numerical. Scatterplots of every variable pair of the data can be seen in the pairs-plot. It is clearly visible that only the variable "chas" is dichotomous with values 0 and 1. The inside pairs correlations or connections differ quite much from each other.


The variable "crim" describes the per capita crime rate by town. Below the summary shows that the crime rate has a large variance. Maybe a better picture of the structure of it we get from the graphical performance. In the scatterplot we have the crime rate on y-axis and it is compared to the median value of owner-occupied homes, variable "medv" in the data. The maximum value of crime rate (88.98) seems to be an outlier better than descriptive data point, because most of the observations are from 0 to about 25. There is some linearity between the home median value and crime rate, but not at the maximum point. It is interesting that when "medv" has it's max value (50), suddenly crime rate gets also increased values. This would mean that in the towns where median value of owner-occupied homes is $50 000, the per capita crime rate is higher than for lower median values between 30 and >50.

```{r}
summary(Boston$medv)
summary(Boston$crim)


p <- ggplot(Boston, aes(x=medv, y=crim)) + geom_point(col="darkorchid1") + ylab("Crime rate per capita") + xlab("Median value of owner-occupied homes in $1000s") + ggtitle("Scatterplot of crime rate")
p + theme_bw()

```


Before linear discriminant analysis we must scale the data and divide it to two subsets; test set and train set. First the scaling;

```{r}
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)

summary(boston_scaled)

```

Scaling the data affected in the level of the variables; now every variable has the mean of 0. Thus they all vary in both sides of the same mean value.

Next we are going to create a categorical variable of the variable "crim" in our scaled data. This is done by using the quantiles as the break points in the categorical variable.

```{r}

scaled_crim <- boston_scaled$crim

# a quantile vector of crim
bins <- quantile(scaled_crim)

# creating the categorical variable
crime <- cut(scaled_crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

table(crime)

# removing original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# adding the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)



```

Now let's divide the data to train and test sets.

```{r}
# number of rows 
n <- nrow(boston_scaled)

# choosing 80% of the rows
ind <- sample(n,  size = n * 0.8)

# train set
train <- boston_scaled[ind,]

# test set 
test <- boston_scaled[-ind,]

```


Now, we will run the linear discriminant analysis using the categorical crime rate as the target variable and all the other variables as predictors.

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit
```

From the results we see for example the propabilities for every group of the variable "crime", then group means and propabilities for every ld model. We see that R has constructed three ld groups, which is one less than the groups of "crime".

Let's then draw the LDA biplot with informative arrows. We use colour groups that are our ld model classes.

```{r}
# lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

plot(lda.fit, dimen = 2, col=rainbow(4, start = 0.7, end = 0.9)[classes], pch=classes)
lda.arrows(lda.fit, myscale = 3, col="darkblue")

```

We see that in our model the 'low', 'med_low' and 'med_high' groups are very near to each other and so the grouping is not that obvious. Also some 'med_high' observations seem to belong to the 'high' group, because they are separated from all other data points. The red arrows demonstrate the direction and volume of the predictive variables.

Before we can use the test data for some further predictions, we should save the crime categories from the test data and then remove the categorical crime variable.

```{r}
# saving the correct classes from test set
correct_classes <- test$crime

# removing the crime variable from test data
test <- dplyr::select(test, -crime)
```

Now we will predict the classes with the LDA model and cross tabulate the results with the crime categories from the test set.

```{r}
lda.pred <- predict(lda.fit, newdata = test)

table(correct = correct_classes, predicted = lda.pred$class)
```

The cross tabulation shows that a large fraction of the observations have been predicted correctly. Still for example 10 obs have been predicted 'med_low' allthough the correct classification is low.


We can now move on to the most interesting part of the chapter, K-means clustering.
For this we reload the Boston dataset and standardize it. After that we can calculate distance measures using the euklidean and manhattan distance.

```{r}
data("Boston")

boston_stand <- scale(Boston)
boston_stand <- as.data.frame(boston_stand)

# Euklidean distance
dist_eu <- dist(boston_stand)
summary(dist_eu)
# Manhattan distance
dist_man <- dist(boston_stand, method="manhattan")
summary(dist_man)

```

Using the euklidean distance measure we run the k-means clustering algorithm on the data. To find the optimal number of clusters we can investigate the within cluster sum of squares (WCSS) measure by drawing a plot of the total WCSS values for each number of clusters.

```{r}
set.seed(123)

# The total within sum of squares
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})

# visualize the results
plot(1:k_max, twcss, type='b', col="darkorchid1", xlab="Number of clusters", ylab="Total WCSS")
axis(1, at=1:10)

km <-kmeans(dist_eu, centers = 4)
```

We find that the most radical change in WCSS is when there are 2 clusters. So we will try K-means with 2 centers.

```{r}
km1 <-kmeans(dist_eu, centers = 2)

```

Last, we can visualize the clusters, here is the graph for 3 clusters.

```{r}
pairs(boston_stand, col=rainbow(4, start = 0.7, end = 0.9)[km1$cluster])
```
















